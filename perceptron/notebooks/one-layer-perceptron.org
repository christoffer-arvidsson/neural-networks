#+TITLE: One Layer Perceptron
#+setupfile: ~/Dropbox/org/orbit/articles/setup_file.org

* Libraries
Import some libraries we'll use
#+begin_src jupyter-python :exports none :kernel ml
%load_ext autoreload
%autoreload 2
import sys
sys.path.append('../src')
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload

#+begin_src jupyter-python :kernel ml
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from model import Perceptron
#+end_src

#+RESULTS:

* Data normalization
Read the datasets with pandas
#+begin_src jupyter-python :kernel ml :file dataset.png :exports both
names = ['x_1', 'x_2', 'y']
training_set = pd.read_csv('../datasets/training_set.csv', sep=',', header=None, names=names)
validation_set = pd.read_csv('../datasets/validation_set.csv', sep=',', header=None, names=names)

g = sns.scatterplot(data=training_set, x='x_1', y='x_2', hue='y', legend=True, s=8.0)
#+end_src

#+RESULTS:
[[file:dataset.png]]

#+attr_latex: :width 0.6\textwidth
#+RESULTS:

Center and standardize the datasets.
#+begin_src jupyter-python :kernel ml
x_train = training_set.values[:,:2]
x_train_normalized = (x_train - x_train.mean()) / x_train.std()
y_train = training_set.values[:,-1]

x_val = validation_set.values[:,:2]
x_val_normalized = (x_val - x_train.mean()) / x_train.std()
y_val = validation_set.values[:,-1]
#+end_src

#+RESULTS:

* Model
Initiate and train the model. Here we use early stopping: stop when the model has not improved for =patience= number of epochs.
#+begin_src jupyter-python :kernel ml :exports both
seed = None
epochs = 1000
learning_rate = 0.01
patience = 50
layer_sizes = [2, 16, 1]
rng = np.random.default_rng(seed)

net = Perceptron(layer_sizes, rng)

net.train(x_train_normalized,
          y_train,
          x_val_normalized,
          y_val,
          epochs,
          learning_rate=learning_rate,
          patience=patience)

#+end_src

#+RESULTS:
: Epoch: 237/1000,	step: 2370000,	energy_train: 2139.2920,	error_val: 0.1246

For simplicity, we'll visualize the results using the validation set rather that a designated test set
#+begin_src jupyter-python :kernel ml :file ../img/perceptron_classification.png :exports both
y_pred = np.zeros_like(y_train)
for i, x in enumerate(x_train_normalized):
    y_pred[i] = np.sign(net.predict(x))

prediction = pd.DataFrame({
    'x_1': x_train_normalized[:,0],
    'x_2': x_train_normalized[:,1],
    'y_true': y_train,
    'y_pred': y_pred,
    'correct': y_train == y_pred
})
g = sns.scatterplot(data=prediction, x='x_1', y='x_2', hue='y_pred', legend=True, s=8.0)
#+end_src

#+RESULTS:
[[file:../img/perceptron_classification.png]]

#+attr_latex: :width 0.6\textwidth
#+RESULTS:

Save the weights and thresholds. Turns out I transposed weight matrixes compared to what was requested in the assignment.
#+begin_src jupyter-python :kernel ml
W1 = net.layers[1].weights
t1 = net.layers[1].thresholds
W2 = net.layers[2].weights
t2 = net.layers[2].thresholds

np.savetxt("saved_models/w1.csv", W1.T, delimiter=',')
np.savetxt("saved_models/t1.csv", t1, delimiter=',')
np.savetxt("saved_models/w2.csv", W2.T, delimiter=',')
np.savetxt("saved_models/t2.csv", t2, delimiter=',')
#+end_src

#+RESULTS:

* Appendix
The model used it the above notebook
#+include: "../src/model.py" src python
