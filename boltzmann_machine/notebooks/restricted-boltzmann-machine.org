#+title: Restricted Boltzmann machine
#+subtitle: Artificial neural networks
#+setupfile: ~/orbit/templates/setup_file.org
#+property: :header-args: :exports both
#+options: date:nil


* Background :noexport:
Solving the XOR problem means for a Restricted Boltzmann machine to sample
patterns according to the truth table, shown in Table [[tbl:xor]]. The model
distribution $P_{model}$ should be close to this distribution.

#+caption: Truth table of the XOR problem
#+attr_latex: :booktabs t
#+name: tbl:xor
| $x_{1}$ | $x_{2}$ | $t$ | P     |
|---------+---------+-----+-------|
|       / |         |   > |       |
|      -1 |      -1 |  -1 | $1/4$ |
|      -1 |       1 |   1 | $1/4$ |
|       1 |      -1 |   1 | $1/4$ |
|       1 |       1 |  -1 | $1/4$ |

* Results
Parameters used to estimate the model distribution is shown in Table [[tbl:params]]. Notably, the lower learning rate helped training the model for small number of hidden neurons.

#+caption: The parameters used in the model.
#+attr_latex: :booktabs t  :width 0.4\textwidth :font \scriptsize
#+name: tbl:params
| Parameter                      |             Value |
|--------------------------------+-------------------|
| /                              |                 < |
| $k$                            |               100 |
| number of minibatches (trials) |              3000 |
| $n_{\text{visible}}$           |                 3 |
| $n_{\text{hidden}}$            | {1,2,3,4,5,6,7,8} |
| learning rate $\eta$           |             0.001 |
| minibatch size                 |                20 |
| generation realisations        |              2000 |
| generation samples/realization |              1000 |
| averaging runs                 |                 3 |

The KL-divergence over number of hidden neurons is shown in Figure [[fig:kl-div]].
We can see that three neurons seems to be the critical number of hidden neurons.
These results do align with the theoretical upper bound, although the
KL-divergence is slightly higher than this upper bound. This could be due to
the CD-k algorithm converging to a suboptimal solution.

#+caption: The KL-divergence over the number of hidden neurons in the Boltzmann machine for the XOR problem. The shaded region is the confidence interval. The dashed line is the theoretical upper bound.
#+attr_org: :width 400
#+attr_latex: :width 0.5\textwidth
#+name: fig:kl-div
[[file:../img/kl_div.png]]

\newpage
* Notebook :appendix:
** RBM implementation
#+include: ../src/model.py src python
** Dataset
#+begin_src python :exports none
import sys
sys.path.append('../src')

%load_ext autoreload
%autoreload 2
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload

#+begin_src python
import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from model import RBM
from multiprocessing import Pool
import os

sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300

dataset = np.array([
    [-1,-1,-1],
    [1,-1,1],
    [-1,1,1],
    [1,1,-1],
])
#+end_src

#+RESULTS:

** Simulation
Define the simulation function
#+begin_src python
def run_simulation(dataset, k, weight_updates, n_visible, n_hidden,
                   learning_rate, batch_size,
                   n_samples, n_realizations):

    machine = RBM(n_visible, n_hidden)

    # Training
    mus = np.random.choice(dataset.shape[0], size=(weight_updates, batch_size), replace=True)
    counts = np.zeros(dataset.shape[0])
    for i in range(weight_updates):
        mu = mus[i]
        machine.run_cd_k(dataset[mu], k=k, learning_rate=learning_rate)

    # Generation
    random_patterns = np.random.choice([-1, 1], size=(n_realizations, n_visible),
                                         replace=True)
    patterns = np.zeros((n_realizations, n_samples, n_visible),
                        dtype=int)
    for r in range(n_realizations):
        patterns[r] = machine.generate(random_patterns[r], n_samples)

    patterns[patterns == -1] = 0
    n_patterns = n_realizations*n_samples
    patterns = patterns.reshape((n_patterns,-1))
    hashes = patterns.dot(1 << np.arange(patterns.shape[-1]-1, -1, -1))
    unique, counts = np.unique(hashes, return_counts=True)

    distribution = np.zeros(2**n_visible)
    distribution[unique] = counts

    p_model = distribution/(n_patterns)

    return machine, p_model

#+end_src

#+RESULTS:

Parameters used.
#+begin_src python
k = 100              # monte carlo iterations
weight_updates = 3000
n_visible = 3        # N
n_hidden = np.arange(1,8+1)
learning_rate = 0.001
batch_size = 20
n_realizations = 2000
n_samples = 1000
num_processes = 12 # Number of threads you have access to on your cpu.
averaging_runs = 3
#+end_src

#+RESULTS:

Let's do some multiprocessing to speed this up. This runs the simulation for
each value of =n_visible=, =averaging_runs= number of times.
#+begin_src python :exports code
def _pool_func(n_hidden):
   print(f'pid: {os.getpid()}\tRunning for n_hidden: {n_hidden}\n')
   return run_simulation(dataset, k, weight_updates, n_visible, n_hidden,
                         learning_rate, batch_size,
                         n_samples, n_realizations)

requests = []
p_models = np.zeros((averaging_runs, len(n_hidden), 2**n_visible), dtype=float)
with Pool(processes=num_processes) as pool:
    for r in range(averaging_runs):
        requests.append([pool.apply_async(_pool_func, (n,)) for n in n_hidden])

    for r in range(averaging_runs):
        results = [req.get() for req in requests[r]]
        # Effectively transposes our list of tuples into a tuple of lists
        _, p_model = map(list, zip(*results))
        p_models[r] = np.array(p_model)

print('Done with simulations.')
#+end_src

#+RESULTS:
#+begin_example
pid: 381610	Running for n_hidden: 2
pid: 381613	Running for n_hidden: 5
pid: 381611	Running for n_hidden: 3
pid: 381609	Running for n_hidden: 1
pid: 381615	Running for n_hidden: 7
pid: 381612	Running for n_hidden: 4
pid: 381617	Running for n_hidden: 1
pid: 381618	Running for n_hidden: 2
pid: 381614	Running for n_hidden: 6
pid: 381616	Running for n_hidden: 8
pid: 381620	Running for n_hidden: 4
pid: 381619	Running for n_hidden: 3












pid: 381617	Running for n_hidden: 5

pid: 381609	Running for n_hidden: 6

pid: 381611	Running for n_hidden: 7

pid: 381619	Running for n_hidden: 8

pid: 381618	Running for n_hidden: 1

pid: 381613	Running for n_hidden: 2

pid: 381612	Running for n_hidden: 3

pid: 381614	Running for n_hidden: 4
pid: 381610	Running for n_hidden: 5


pid: 381615	Running for n_hidden: 6

pid: 381616	Running for n_hidden: 7

pid: 381620	Running for n_hidden: 8

Done with simulations.
#+end_example
Now plot the KL-divergence

#+name: kl-div
#+begin_src python :file ../img/kl_div.png :exports both
xor = [0,3,5,6] # Represents indices for the xor patterns among random ones
p_data = np.zeros((2**n_visible))
p_data[xor] = 1/4

def kl_divergence_bound(n, m):
    return np.log(2) * (n - np.floor(np.log2(m + 1)) - (m+1)/(2**(np.floor(np.log2(m+1)))))

samples = []
for r in range(averaging_runs):
    for i, p_model in enumerate(p_models[r]):
        kl_divergence = np.sum(p_data[xor] * np.log(p_data[xor] / p_model[xor]))
        samples.append((r, n_hidden[i], kl_divergence))

nonzero_m = n_hidden[n_hidden < 2**(n_visible - 1) -1]
theoretical = np.zeros(len(n_hidden))
theoretical[n_hidden < 2**(n_visible - 1) -1] = kl_divergence_bound(n_visible, nonzero_m)

data = pd.DataFrame(samples, columns=['run', 'hidden_neurons', 'kl_divergence'])
g = sns.lineplot(x='hidden_neurons', y='kl_divergence', data=data, color='black', label='Average')
sns.lineplot(x=n_hidden, y=theoretical, ax=g, color='black', linestyle='--', label='Theoretical')
g.set_xlabel('$M$ (hidden neurons)')
g.set_ylabel(r'$D_{KL}$')
g.set_title('Kullback-Leibler divergence')
plt.show()
#+end_src

#+attr_org: :width 400
#+RESULTS: kl-div
[[file:../img/kl_div.png]]

* Checking distributions :noexport:
#+begin_src python :session test
import sys
sys.path.append('../src')

%load_ext autoreload
%autoreload 2

import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from model import RBM
from multiprocessing import Pool
import os

sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300

dataset = np.array([
    [-1,-1,-1],
    [1,-1,1],
    [-1,1,1],
    [1,1,-1],
])

batch_size = 20
n_visible = 3
n_hidden = 8
k = 100
eta=0.1
n_realizations = 100
n_samples = 100
weight_updates = 10

batch_indices = np.random.choice(dataset.shape[0], size=(n_realizations, weight_updates, batch_size,))
noise = np.random.choice([1,-1], size=(n_realizations, n_visible))

samples = np.zeros((n_realizations, n_samples, n_visible), dtype=int)
for r in range(n_realizations):
    m = RBM(n_visible, n_hidden)

    for i in range(weight_updates):
        batch = dataset[batch_indices[r, i]]
        m.run_cd_k(batch, k=k, learning_rate=eta)

    samples[r] = m.generate(noise[r], n_samples)

samples[samples == -1] = 0
samples = samples.reshape((-1, n_visible))

hashes = samples.dot(1 << np.arange(samples.shape[-1]-1, -1, -1))
unique, counts = np.unique(hashes, return_counts=True)

distribution = np.zeros(2**n_visible)
distribution[unique] = counts

p_model = distribution/(n_samples*n_realizations)
print('Done')
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Done

#+name: fig:test2
#+begin_src python :session test
plt.bar(np.arange(2**3), p_model)
plt.xticks(np.arange(2**3), ["000","001","010","011","100","101","110","111"])
plt.show()
#+end_src

#+attr_org: :width 500
#+RESULTS: fig:test2
[[file:./.ob-jupyter/caf355d6279493e6cde9f09a7bbfeeba3763f047.png]]
